#Script S3. Script for the ancestral range reconstruction analysis performed in RevBayes using DEC with a time-stratified model (M1) with two epochs or time slices (TS) with different transition probabilities matrices.

################################################################################
#!/usr/bin/rb

# filenames
range_fn = "data/Cladonia_distribution_5areas.nex"
tree_fn  = "data/Cladonia_RB_nowides.tre"
out_fn   = "output_simple/output_10000gen_simple-nodist_BF"
geo_fn   = "data/cladonia"



# read binary (01) presence-absence range data
dat_range_01 = readDiscreteCharacterData(range_fn)

# convert binary ranges into NaturalNumbers
dat_range_n = formatDiscreteCharacterData(dat_range_01, "DEC")

#Check size of each vector
dat_range_n.size()
dat_range_01.size()

# compare characters for two taxa
dat_range_01[1]
dat_range_n[1]

# data dimensions
n_areas  = dat_range_01.nchar()
n_states = floor(2^n_areas)

#There are 32 ranges or states for 9 areas (2exp(6)). We are going to reduce the size of the Q matrix by applying constraints
max_areas <- 4
n_states <- 0
for (k in 0:max_areas) n_states += choose(n_areas,k)

#Now number of states (n_states) is 511: equivalent to a sum of combinatorial elements (5 "chooses" 1 + 5 "chooses" 2 + ...)
#Notice that "empty range" is a state in the model!! So we have 511 instead of 512
#Then use the new "n_states" to format the dataset for the reduced state space

dat_range_n = formatDiscreteCharacterData(dat_range_01, "DEC", n_states)

# get the converted state descriptions (OBS: The sample.txt does not consider max_size argument) 
state_desc = dat_range_n.getStateDescriptions()

# write the state descriptions to file
state_desc_str = "state,range\n"
for (i in 1:state_desc.size())
{
    state_desc_str += (i-1) + "," + state_desc[i] + "\n"
}
write(state_desc_str, file=out_fn+".state_labels.txt")

# helper variables
moves = VectorMoves()
monitors = VectorMonitors()
n_gen = 10000


###############
# Tree models #
###############

# read tree
tree <- readTrees(tree_fn)[1]


#######################
# Biogeography models #
#######################

# Set the biogeographic event rate multiplier. This assigns the migration rate baseline for the dispersal and extinction rate instantaneous Q matrix.
# In the tutorial example, this is set to be quite broad or uninformative. "range_bg" is set as a uniform distribution bounded 
# between 0.0001 (10exp(-4)) and 100 (10exp(2)), with an initial value of 0.01 (10exp(-2)).
# Here, we are going to use narrower, biologically more realistic priors. This is also important to avoid numerical overflow in estimating
# Bayes Factors by path sampling: as the sampler gets closer to the prior and further away from the posterior, where the data stops being informative,
# the combination of parameter values sampled from the prior becomes more "unlikely", which results in extremely low likelihoods even after
# log scaling, and eventually in numerical overflow. 
# To establish more realistic priors, we can use the posterior estimates from an initial, test analysis with broader priors.

rate_bg ~ dnLoguniform(1E-4,1E-1)
rate_bg.setValue(1E-2)
moves.append( mvScale(rate_bg, weight=4) )


# the relative dispersal rate
dispersal_rate <- 1.0

##############################################
# The geographical distance scaling factor. The "distance_scale" parameter (a) is used 
# below to scale the baseline dispersal rate by the inverse of the geographic distance.
# If (a) is 0 (no scaling), then the dispersal rate is equal for all areas.
# This code below can be commented out if no distance correction is applied.

#distance_scale ~ dnUnif(0,20)
#distance_scale.setValue(0.01)
#moves[mvi++] = mvScale(distance_scale, weight=3)

# Create the dispersal rate matrix. 

# And we build the relative dispersal rate matrix
for (i in 1:n_areas) {
    for (j in 1:n_areas) {
        dr[i][j] <- dispersal_rate
    }
}

# then the relative extirpation rate (or per-area extinction rates)
# We create a log normal prior with SD = 0.5 and log mean = -0.125,  95% range (0.388, 2.01) Median=0.882
log_sd <- 0.5
log_mean <- ln(1) - 0.5*log_sd^2



extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)
moves.append( mvScale(extirpation_rate, weight=2) )

# Build the relative extirpation rate matrix
for (i in 1:n_areas) {
    for (j in 1:n_areas) {
        er[i][j] <- 0.0       
    }
    er[i][i] := extirpation_rate
}

# Build the DEC rate matrix
Q_DEC := fnDECRateMatrix(dispersalRates=dr,
                         extirpationRates=er,
                         maxRangeSize=max_areas)


##############################################
# Build cladogenetic transition probabilities. Only sympatry (wide or peripatry, narrow) and allopatry (vicariance) are allowed.
clado_event_types <- [ "s", "a" ]
clado_type_probs <- simplex(1,1)

# We assign a simplex that sums to 1, but the proportion of the prior for each event is different.
#p_sympatry ~ dnUniform(0,1)
#p_allopatry := abs(1.0 - p_sympatry)
#clado_type_probs := simplex(p_sympatry, p_allopatry)
#moves.append( mvSlide(p_sympatry, weight=2) )

P_DEC := fnDECCladoProbs(eventProbs=clado_type_probs,
                         eventTypes=clado_event_types,
                         numCharacters=n_areas,
                         maxRangeSize=max_areas)

# Construct the phylogenetic CTMC with cladogenetic events
m_bg ~ dnPhyloCTMCClado(tree=tree,
                           Q=Q_DEC,
                           cladoProbs=P_DEC,
                           branchRates=rate_bg,
                           type="NaturalNumbers",
                           nSites=1)
    
# attach the range data
m_bg.clamp(dat_range_n)

############
# Monitors #
############

monitors.append( mnScreen(printgen=10, rate_bg, extirpation_rate) )
monitors.append( mnModel(file=out_fn+".model.log", printgen=10) )
monitors.append( mnFile(tree, filename=out_fn+".tre", printgen=10) )
monitors.append( mnJointConditionalAncestralState(tree=tree,
                                                  ctmc=m_bg,
                                                  type="NaturalNumbers",
                                                  withTips=true,
                                                  withStartStates=true,
                                                  filename=out_fn+".states.log",
                                                  printgen=10) )
monitors.append( mnStochasticCharacterMap(ctmc=m_bg,
                                          filename=out_fn+".stoch.log",
                                          printgen=10) )

############
# Analysis #
############


# build the model analysis object from the model graph
mymodel = model(m_bg)

# create the MCMC analysis object
mymcmc = mcmc(mymodel, monitors, moves)

# run the MCMC analysis
mymcmc.run(n_gen)


#### Bayes Factor Comparison: Include code for path and stepping-stone sampling ####

### Compute power posterior distributions
# pow_p = powerPosterior(mymodel, moves, monitors, "outputPPsimple/pow_p_DEC_simple_BF.out", cats=100, sampleFreq=10)
# pow_p.burnin(generations=10000,tuningInterval=100)
# pow_p.run(generations=1000)

### Use stepping-stone sampling to calculate marginal likelihoods

# ss = steppingStoneSampler(file="outputPPsimple/pow_p_DEC_simple_BF.out", powerColumnName="power",
# likelihoodColumnName="likelihood")
# ss.marginal()
### Use path-sampling to calculate marginal likelihoods
# ps = pathSampler(file="outputPPsimple/pow_p_DEC_simple_BF.out", powerColumnName="power", likelihoodColumnName="likelihood")
# ps.marginal()

# exit
# quit()






